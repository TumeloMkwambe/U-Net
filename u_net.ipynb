{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b50fd05-3c0e-45de-b6d4-0c956e2d0a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7159e41-9eec-494f-a946-688bb3c0a491",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd659d6b-a5a8-4510-af02-f4d9aca459bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a2022-4a09-4540-95f1-88acc3ec458d",
   "metadata": {},
   "source": [
    "### U-Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018b69dc-6ea9-460c-bf69-539d9f229210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoConvolutions(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, stride = 1, padding = (1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, stride = 1, padding = (1,1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \n",
    "        output = self.block(input_)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804b3980-dbb7-41af-9319-bd12fff1479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.TwoConvolutions = TwoConvolutions(in_channels, out_channels)\n",
    "        self.max_pooling = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        skip_features = self.TwoConvolutions(image)\n",
    "        features = self.max_pooling(skip_features)\n",
    "        \n",
    "        return features, skip_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "609d8ed7-abd0-43d5-b75e-9d778bbe0d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.upConvolution = nn.ConvTranspose2d(in_channels, out_channels, kernel_size = 2, stride = 2)\n",
    "        self.TwoConvolutions = TwoConvolutions(out_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, input_, skip_input):\n",
    "        \n",
    "        features = self.upConvolution(input_)\n",
    "        features = torch.cat([features, skip_input], dim = 1)\n",
    "        features = self.TwoConvolutions(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01240c46-84d0-4c96-8b8d-be06c73685cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, depth):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.channels = [in_channels] + [64 * (2 ** i) for i in range(depth + 1)]\n",
    "\n",
    "        self.Encoder = nn.ModuleList([\n",
    "            EncoderBlock(self.channels[i], self.channels[i+1]) for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.Bottleneck = TwoConvolutions(self.channels[depth], self.channels[depth + 1])\n",
    "\n",
    "        self.channels.reverse()\n",
    "        self.channels.pop()\n",
    "\n",
    "        self.Decoder = nn.ModuleList([\n",
    "            DecoderBlock(self.channels[i], self.channels[i+1]) for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.FinalConvolution = nn.Conv2d(in_channels = self.channels[-1], out_channels = out_channels, kernel_size = 3, stride = 1, padding = (1,1))\n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        encoder_features = []\n",
    "        features = image\n",
    "\n",
    "        for block in self.Encoder:\n",
    "            features, skip_features = block(features)\n",
    "            encoder_features.append(skip_features)\n",
    "\n",
    "        features = self.Bottleneck(features)\n",
    "        encoder_features.reverse()\n",
    "\n",
    "        for idx, block in enumerate(self.Decoder):\n",
    "            features = block(features, encoder_features[idx])\n",
    "\n",
    "        mask = self.FinalConvolution(features)\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7742401-cdc3-4570-b0cf-f2634bc990ba",
   "metadata": {},
   "source": [
    "### Optimization and Performance Report Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08c146b4-3279-4905-ba60-9e1d74fff396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, training_data, validation_data, batch = 1, learning_rate = 1e-2, num_epochs = 10):\n",
    "\n",
    "    dataset = ImageDataset(training_data)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch, shuffle = True)\n",
    "    model = model.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for image, mask in dataloader:            \n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(image)\n",
    "            loss = loss_function(logits, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch: {epoch} | Loss: {average_loss}')\n",
    "        performance_report(\"Validation\", model, validation_data, batch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adf89150-5d54-4c5f-a02c-e7cbc3a3ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_report(type_, model, data, batch_size):\n",
    "\n",
    "    dataset = ImageDataset(data)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle = False)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for image, mask in dataloader:\n",
    "            image = image.to(device)\n",
    "            mask = mask.to(device)\n",
    "            logits = model(image)\n",
    "            loss = loss_function(logits, mask)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    print(f\"{type_} | Loss: {average_loss} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801b97d-3d71-448d-83ff-50aaa6d65c1d",
   "metadata": {},
   "source": [
    "### Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9c67b3-887c-4da5-8868-8f3fdf63bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.data[idx]\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46e71f7a-9c31-437a-b551-6802f63e1890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(type_, image_nums):\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for image_num in image_nums:\n",
    "        \n",
    "        image = cv2.imread(f'./images-1024x768/{type_}/image-{image_num}.png')\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = torch.tensor(image, dtype = torch.float32)\n",
    "        image = image / 255.0\n",
    "        \n",
    "        mask = cv2.imread(f'./masks-1024x768/{type_}/mask-{image_num}.png', cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask > 0).astype('int')\n",
    "        mask = torch.tensor(mask, dtype = torch.long)\n",
    "        data.append([image, mask])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0f28376-37d0-464f-b899-ac3e48996587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(image):\n",
    "\n",
    "    if hasattr(image, \"detach\"):\n",
    "        image = image.detach().cpu().numpy()\n",
    "\n",
    "    plt.imshow(image, cmap = \"binary\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"View Image\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a595f8c-dacf-422d-97be-b47dad72a641",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90c092bd-0c75-4420-bd34-d3d8c8b380c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prep_data('train', [2, 7, 10, 12, 21, 24, 27, 28, 30, 43])\n",
    "val_data = prep_data('val', [1, 11, 22, 32])\n",
    "test_data = prep_data('test', [4, 16, 29, 36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb5e2c6-a476-4eda-bdf1-589109bc4f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
